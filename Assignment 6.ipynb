{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650d9e3d-e832-433f-960b-6351e677dd50",
   "metadata": {},
   "source": [
    "\n",
    "1. Data Ingestion Pipeline:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7c3a69-0432-4358-8275-c488cc600af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 .Design a data ingestion pipeline that collects and stores data from various sources such as databases, APIs, and streaming platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e217e0-2187-48fa-8ff4-3f0b2bf173f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = A data ingestion pipeline in the context of machine learning refers to the process of collecting, acquiring,\n",
    "     and preparing data for analysis. It involves the extraction, transformation, and loading (ETL) of data from various\n",
    "     sources into a format suitable for machine learning tasks.\n",
    "        \n",
    "    APIs,and ingestion pipeline\n",
    "    a. Data collection: Gathering data from different sources such as databases, APIs, files, or streaming platforms.\n",
    "    b. Data extraction: Extracting the relevant data from the source systems or files.\n",
    "    c. Data preprocessing: Cleaning and transforming the data to handle missing values, outliers, and inconsistencies.\n",
    "    d. Data integration: Combining data from multiple sources into a unified format.\n",
    "    e. Data loading: Storing the processed data into a suitable storage system for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b6be3d-1f1a-424c-b287-dcf8b185db9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48a9baf-e966-4eb8-ba72-92c9aa4929a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2.Implement a real-time data ingestion pipeline for processing sensor data from IoT devices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54b1fa-7ef4-40a6-bba0-34f89e527aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "     a. Event-driven architectures: Implementing event-driven systems that react to incoming data events and trigger corresponding actions.\n",
    "     b. Stream processing frameworks: Utilizing frameworks like Apache Kafka, Apache Flink,\n",
    "        or Apache Storm to process and analyze streaming data in real-time.\n",
    "    c. Real-time data integration: Implementing connectors or APIs to seamlessly integrate real-time data sources with the ingestion pipeline.\n",
    "    d. Low-latency data processing: Optimizing the data processing infrastructure to minimize latency and enable real-time ingestion and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3902c04b-bc6b-4c13-8b88-f123b05df276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fad703-c379-45cf-bda6-5305858b5830",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3.Develop a data ingestion pipeline that handles data from different file formats (CSV, JSON, etc.) and performs data validation and cleansing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa6d7d6-78d3-49e9-a9e9-b24ea431d312",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = a. Distributed processing: Utilizing distributed computing frameworks like Apache Hadoop or Apache Spark to parallelize the ingestion process.\n",
    "    b. Incremental loading: Loading only the new or updated data instead of the entire dataset to minimize resource usage and improve efficiency.\n",
    "    c. Data partitioning: Dividing the data into smaller partitions to distribute the workload and enable parallel processing.\n",
    "    d. Compression techniques: Using compression algorithms to reduce the storage requirements and speed up the data transfer process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac7529-31d2-4c26-9ea1-1a582e584e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c04f5152-8b4a-4b10-bb6a-eab7a531d744",
   "metadata": {},
   "source": [
    "2. Model Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb37848-4ac2-4324-b405-efc6b8d6a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Build a machine learning model to predict customer churn based on a given dataset. \n",
    "      Train the model using appropriate algorithms and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bbd09-0dc6-4610-90de-4c214d0dce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =  Logistic Regression.\n",
    "     Logistic regression is a machine learning model that is widely used in cases where the \n",
    "      target variables can take only one of the two possible values. \n",
    "  \n",
    "    a. Data preparation: Preparing the dataset by cleaning, transforming, and preprocessing the data.\n",
    "   b. Splitting the data: Dividing the dataset into training and validation subsets to evaluate the model's performance.\n",
    "   c. Model selection: Choosing an appropriate algorithm or model architecture based on the problem type and requirements.\n",
    "   d. Model initialization: Initializing the models parameters or weights.\n",
    "   e. Model training: Optimizing the models parameters using an optimization algorithm such as gradient descent.\n",
    "   f. Model evaluation: Assessing the trained models performance on the validation set.\n",
    "   g. Iterative refinement: Iterating the training process by adjusting hyperparameters, changing model architectures, or modifying the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abaa67e-2ee0-4013-93cd-beae594ac710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec061739-ae7d-4bef-9319-e20c893ba0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "  Q2. Develop a model training pipeline that incorporates feature engineering techniques such as one-hot encoding, \n",
    "    feature scaling, and dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4258076-bbea-4fb3-82da-861a654ccd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =  Feature engineering involves creating new features or transforming existing features to enhance the models predictive power.\n",
    "     Techniques include:\n",
    "    1. Polynomial features: Creating interaction terms or polynomial combinations of features.\n",
    "   2. Feature scaling: Scaling features to a specific range, such as min-max scaling or standardization.\n",
    "    3. Feature encoding: Encoding categorical variables using techniques like one-hot encoding or ordinal encoding.\n",
    "   4. Dimensionality reduction: Reducing the dimensionality of the feature space using techniques like PCA or t-SNE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be81a2d-e19c-45ac-aa42-e9aa351e7e65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7915090-d894-41b1-b6f9-29049b218540",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3.Train a deep learning model for image classification using transfer learning and fine-tuning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e0d398-e171-4a26-afcd-c7474ffebee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    Fine-tuning is a common technique for transfer learning. \n",
    "    The target model copies all model designs with their parameters from the source model except the output layer, \n",
    "    and fine-tunes these parameters based on the target dataset. \n",
    "    In contrast, the output layer of the target model needs to be trained from scratch.\n",
    "    either use the pretrained model as is or use transfer learning to customize this model to a given task. \n",
    "    The intuition behind transfer learning for image classification is that if a model is trained on a large\n",
    "    and general enough dataset, this model will effectively serve as a generic model of the visual world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81960c-cb55-4cb8-a4db-37f8106831d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2622d6a6-f414-4786-9f7c-6fbd6c62ff4a",
   "metadata": {},
   "source": [
    "3. Model Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62784a6b-4e4a-4403-b0e5-d3b611aba669",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1.Implement cross-validation to evaluate the performance of a regression model for predicting housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9d786c-9d83-41a3-9993-490dc5a4b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Model validation refers to the process of assessing the performance and reliability \n",
    "of a trained model using independent data that was not used during the model training phase. \n",
    "It is important because it provides an estimation of how well the model is likely to perform on unseen\n",
    "data and helps in selecting the best model for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34edae4e-830b-41ff-95a8-fe286a770eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba72c30-7a33-41f4-8701-c68f94bb089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2.Perform model validation using different evaluation metrics such as accuracy, precision, recall, and F1 score for a binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070347aa-edaa-464e-a4e6-1347a54228ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "    Overfitting occurs when a model learns the training data too well, capturing noise or irrelevant patterns.\n",
    "     Underfitting, on the other hand, occurs when a model fails to capture the underlying patterns in the data. \n",
    "    To handle overfitting and underfitting during model validation:\n",
    "\n",
    "   - Overfitting: Regularization techniques like L1 or L2 regularization, dropout, or early stopping can be employed\n",
    "   . Additionally, reducing model complexity, increasing the amount of training data, or applying data augmentation\n",
    "     techniques can help alleviate overfitting.\n",
    "   - Underfitting: Increasing model complexity, adding more features,\n",
    "    or choosing a more expressive model architecture can help mitigate underfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee217a6-474c-4585-b8bf-7d6c3c8ad732",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e994d0-6322-4090-89f8-1684df5132df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3.Design a model validation strategy that incorporates stratified sampling to handle imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061fabc-83aa-4cb0-9f24-9ce0cb2ba189",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS =\n",
    "     Model evaluation involves using the available dataset to fit a model and estimate its performance when making predictions on unseen examples.\n",
    "    It is a challenging problem as both the training dataset used to fit the model and the test set used to evaluate it must be sufficiently \n",
    "    large and representative of the underlying problem so that the resulting estimate of model performance is not too optimistic or pessimistic.\n",
    "    The two most common approaches used for model evaluation are the train/test split and the k-fold cross-validation procedure. \n",
    "    Both approaches can be very effective in general, although they can result in misleading results and potentially fail when used on\n",
    "    classification problems with a severe class imbalance. Instead, the techniques must be modified to stratify the sampling by the class \n",
    "    label, called stratified train-test split or stratified k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8f302-34bb-4c2a-8c3f-89bfec0b44a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c0bc75f-fad2-4265-9691-481764a9b3d4",
   "metadata": {},
   "source": [
    "4. Deployment Strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34da9497-996d-47ed-b68b-4f808d5a8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1.Create a deployment strategy for a machine learning model that provides real-time recommendations based on user interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b122a0fd-2243-428e-b471-6188dbe0c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = \n",
    "   - Scalability: The environment should be capable of handling varying workloads and scaling up as the demand for predictions increases.\n",
    "   - Security: The deployment environment should provide adequate security measures to protect sensitive data and prevent unauthorized access.\n",
    "   - Compatibility: The environment should be compatible with the technology stack used for model training and should support the required frameworks and libraries.\n",
    "   - Integration: The deployment environment should allow seamless integration with existing systems and data sources.\n",
    "   - Cost: The deployment environment should be cost-effective and provide efficient resource utilization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b03af-dac6-4bff-9572-57bca23f4222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c553e2-9b02-419d-ba79-277ae0b8eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2.Develop a deployment pipeline that automates the process of deploying machine learning models to cloud platforms such as AWS or Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880920e-a62d-48e9-9d7d-39d424095e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load data and split into train/test sets\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Select first 4 features\n",
    "X = X[:,:4]\n",
    "\n",
    "# Split dataset into training and evaluation subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Create and train model\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63423d8-ed3e-49c9-9d72-db136947df6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16393a-7baa-41a9-8274-3d4c030b714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3.Design a monitoring and maintenance strategy for deployed models to ensure their performance and reliability over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de07845b-af1a-4bd7-a0d6-219691df205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANS = Data quality monitoring is the first line of defense for production machine learning systems.\n",
    "     By looking at the data, you can catch many issues before they hit the actual model performance. \n",
    "    You can, and should do that for every model. \n",
    "    It is a basic health check, similar to latency or memory monitoring.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0573976-cbf3-43b1-8f24-1408a0a80afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6de36-0fb3-4111-8319-4323d731f9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
